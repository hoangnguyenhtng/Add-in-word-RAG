# test_pinecone_grpc_ready.py
import os, time
from typing import List
from pinecone.grpc import PineconeGRPC as Pinecone
from pinecone import ServerlessSpec
import google.generativeai as genai

PINECONE_API_KEY = os.getenv("PINECONE_API_KEY") or "PASTE_PINECONE_KEY"
GOOGLE_API_KEY   = os.getenv("GOOGLE_API_KEY")   or "PASTE_GEMINI_KEY"

INDEX_NAME = "docs-example-classic"
REGION     = "us-east-1"
NAMESPACE  = "example-namespace"

EMBED_MODEL = "models/text-embedding-004"  # 768-D
DIMENSION   = 768

def wait_index_ready(pc, name, timeout=120):
    start = time.time()
    while True:
        info = pc.describe_index(name).to_dict()
        state = info.get("status", {}).get("state")
        if state == "Ready":
            print(f"âœ… Index ready: {state}")
            return
        if time.time() - start > timeout:
            raise TimeoutError(f"Index not ready after {timeout}s. State={state}")
        print(f"â³ Waiting index ready... state={state}")
        time.sleep(2)

def embed_texts(texts: List[str]) -> List[List[float]]:
    out = []
    for t in texts:
        r = genai.embed_content(model=EMBED_MODEL, content=t, task_type="retrieval_document")
        emb = getattr(r, "embedding", None) or r.get("embedding")
        if not emb:
            raise RuntimeError("Gemini embedding failed")
        out.append(emb)
    return out

def main():
    genai.configure(api_key=GOOGLE_API_KEY)
    pc = Pinecone(api_key=PINECONE_API_KEY)

    # 1) Táº¡o index náº¿u thiáº¿u
    if not pc.has_index(INDEX_NAME):
        print(f"ğŸ†• Creating index '{INDEX_NAME}' (dim={DIMENSION}, cosine)â€¦")
        pc.create_index(
            name=INDEX_NAME,
            dimension=DIMENSION,
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region=REGION),
            deletion_protection="disabled",
        )
    else:
        print(f"âœ… Index '{INDEX_NAME}' already exists.")

    # 2) Chá» READY (ráº¥t quan trá»ng)
    wait_index_ready(pc, INDEX_NAME)

    index = pc.Index(INDEX_NAME)

    # 3) Dá»n namespace test (Ä‘á»ƒ Ä‘áº¿m vector dá»…)
    try:
        index.delete(delete_all=True, namespace=NAMESPACE)
        time.sleep(1)
    except Exception:
        pass

    # 4) Embed + upsert
    doc_id = "grpc_demo_doc"
    passages = [
        "Luáº­t Giao thÃ´ng 2024 quy Ä‘á»‹nh tá»‘c Ä‘á»™ tá»‘i Ä‘a trong khu dÃ¢n cÆ° lÃ  50 km/h.",
        "Theo quy Ä‘á»‹nh nÄƒm 2016, tá»‘c Ä‘á»™ tá»‘i Ä‘a trong khu dÃ¢n cÆ° lÃ  60 km/h."
    ]
    print("ğŸ§  Embedding passages with Geminiâ€¦")
    vecs = embed_texts(passages)

    print("ğŸ“¤ Upserting vectors via gRPCâ€¦")
    upsert_payload = [
        {
            "id": f"{doc_id}_p{i}",
            "values": vecs[i],  # 768 floats
            "metadata": {"doc_id": doc_id, "chunk_id": i, "text": passages[i], "kind": "kb_document"},
        }
        for i in range(len(passages))
    ]
    resp = index.upsert(vectors=upsert_payload, namespace=NAMESPACE)
    print("ğŸ” UpsertResponse:", resp)

    # 5) Chá» Ä‘á»“ng bá»™ vector_count tÄƒng
    for _ in range(10):
        stats = index.describe_index_stats()
        count = stats.namespaces.get(NAMESPACE, {}).get("vector_count", 0)
        print(f"ğŸ“ˆ vector_count in '{NAMESPACE}':", count)
        if count >= len(passages):
            break
        time.sleep(1.5)

    # 6) Embed cÃ¢u há»i + query
    question = "Tá»‘c Ä‘á»™ tá»‘i Ä‘a trong khu dÃ¢n cÆ° hiá»‡n nay lÃ  bao nhiÃªu?"
    print("ğŸ” Embedding question & queryingâ€¦")
    q_vec = embed_texts([question])[0]
    res = index.query(
        vector=q_vec,
        top_k=3,
        include_metadata=True,
        namespace=NAMESPACE,
    )

    print("ğŸ“Š Matches:")
    if not res.matches:
        print("âš ï¸ No matches. Check dimension, namespace, or vector_count above.")
    else:
        for m in res.matches:
            md = m.metadata or {}
            print(f"- score={m.score:.3f} id={m.id}")
            print("  text:", md.get("text", ""))
            print("  ---")

if __name__ == "__main__":
    main()
